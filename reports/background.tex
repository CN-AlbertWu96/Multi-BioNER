
\subsection{Problem definition of Name Entity Recognition (NER)}

 Given a word sequence $\textbf{w} = \{w_1, w_2, ..., w_n\}$, we should assign the best label sequence $\textbf{l} = \{l_1, l_2, ..., l_n\}, l_i \in L$ to each word separately, where L includes every possible labels according to IOBES schemes (see \ref{exp:iobes}). For example, if we are given a sentence "Selegiline - induced postural hypotension...", the best label sequence should be like "S-Chemical O O B-Disease E-Disease...". Because in this sentence, "selegiline" is a kind of chemical and "postural hypotension" is a disease, whereas other words are out of chunks.

\subsection{Long Short-Term Memory (LSTM)}
\begin{figure}[h]
\input{lstm_figure.tex}
\caption{Architecture of Long Short-Term Memory Neural Network.}
\label{lstm_arch}
\end{figure}
Recurrent neural networks (RNNs) are a kind of neural networks that conduct operations upon sequential data. The input to a RNN is a sequence of vectors ($x_1, x_2, ..., x_t$), where each vector $x_i$ is a representation of an element in the input sequence. The output generated by the network is another sequence ($h_1, h_2, ..., h_t$) that represents some information about sequence at each step, where each vector $h_i$ is a hidden state vector. Although RNNs can learn long dependencies, previous work has also found out that RNNs, in practice, appears to be biased towards the most recent inputs~\citep{bengio1994} because when training a RNN using back-propagation, the gradients can tend to zero or infinity due to the usages of numbers with finite precision.

Long short-term memory (LSTM) neural network is a designated type of RNN that aims to model dependencies between sequence elements~\citep{lstm1997}. Figure~\ref{lstm_arch} illustrates an architecture of LSTM neural network. At step $t$ of recurrent calculation, $c_t$ represents memory stored in cell, $h_t$ represents the output hidden state, $x_t$ is the input vector, $\sigma$ denotes element-wise sigmoid function, $tanh$ denotes element-wise hyperbolic tangent function, $+$ and $\times$ denotes element-wise summation and multiplication.  At step $t$ of the recurrent calculation, the inputs of the network are $x_t$, $c_\text{t-1}$, $h_\text{t-1}$ and the outputs of the network are $c_t$, $h_t$. Previous work~\citep{wang2018cross} uses the following implementation:
\begin{align*} 
i_t &= \sigma(W^ix_t + U^ih_{t-1} + b^i)\\
f_t &= \sigma(W^fx_t + U^fh_{t-1} + b^f)\\
o_t &= \sigma(W^ox_t + U^oh_{t-1} + b^o)\\
g_t &= tanh(W^gx_t + U^oh_{g-1} + b^g)\\
c_t &= f_t \odot c_{t-1} + i_t \odot g_t \\
h_t &= o_t \odot tanh(c_t)'
\end{align*}
where $i_t, f_t$ and $o_t$ are referred as input, forget, and output gates. Initially, $h_0$ and $c_0$ are zero vectors and the parameters to train are $W^j, U^j$ and $b^j$ for $j\in\{i,f,o,g\}$.

\subsection{Bi-directional Long Short-Term Memory (BiLSTM)}
LSTM model computes a sequence of hidden state vectors that represents the previous context of the sentence at each word. It is intutive that the subsequent context can also provide useful information.

Inspired by this idea, Bi-directional Long Short-Term Memory (BiLSTM) improves the LSTM model by feeding inputs twice with different directions, one in the original direction, the other in the reversed direction~\citep{bilstm}. Then, the outputs from each directions are aggregated together as the final output, which captures the dependencies from not only the previous elements but also the subsequent elements of the sequence.


\subsection{Bi-directional Long Short-Term Memory-Conditional Random Field (BiLSTM-CRF)}
A very simple tagging model can use the output hidden state vectors as features to make independent tagging decisions. However, a limitation of this strategy appears when the dependencies across the labels are very strong. NER is a typical example of such task because the grammatical structure has significant affects on the labels but the structure will not be modeled with independent assumptions. It is useful and important to model the dependencies across output labels. Previous work utilizes a conditional random field to conduct this task and has shown a promising result~\citep{lafferty2001conditional}. For an input sequence, $X$ = ($x_1, x_2, ..., x_n$), and output sequence $y$ = ($y_1, y_2, ..., y_n$), a score is defined as:\\
\begin{align*}
    s(X,y)= \sum_{i=0}^{n} A_{y_i, y_{i+1}} + \sum_{i=1}^{n} P_{i, y_i}
\end{align*}
where $n$ is the length of the sequence, $k$ is the number of distinct labels, $P$ is an $n \times k$ matrix of the output from the BiLSTM layer, $A_{i,j}$ is the transition probability from label $i$ to label $j$. There are two additional labels that represents \textit{start} and \textit{end} of the sequences and therefore A is a matrix with dimension $(k+2)\times(k+2)$.

The training process maximizes the log-probability of the correct label sequence: 
\begin{align*}
    log(p(y|X)) = s(X,y) - log(\sum_{\widetilde{y}\in Y_X} e^{(s(X, \widetilde{y})))}
\end{align*}
where $Y_X$ are defined as all possible label sequences for a input sequence $X$. During the prediction stage, the model predicts the output sequence by:

\begin{align*}
   y^* = \argmax_{\widetilde{y}\in Y_X} s(X, \widetilde{y})
\end{align*}
\subsection{Attention}
Inspired by the observations that certain alignment exists between the input sequence and output sequence, which means that each step of generating a token is greatly related to a certain part of the input sequence, attention mechanism aims to allow the model to refer back to the input sequence~\citep{young2018recent}. An attention network keeps a set of hidden state representations that scale with the size of the input sequence. The model performs selection over the representations to allow the model to maintain a variable-length memory~\citep{structatten}. 

For input sequence $(x_1, x_2, ..., x_n)$, let $z$ be a categorical latent variable with sample space \{1, 2, ..., n\} that represents the selection among input, and let $q$ be a query. The mechanism aims to generate a context $c$ with the input sequence and a query, by accessing to a distribution $z \mathtt{\sim} p(z|x, q)$ that can be derived from applying softmax function to vectors of alignment scores, where it conditions $p$ on the input $x$ and a query $q$. Then, the context over a sequence is defined as:
\begin{align*}
    c = \mathbb{E}_{z \mathtt{\sim} p(z|x, q)} [f(x, z)]
\end{align*}
where $f(x, z)$ is a compatibility function. Two types of compatibility functions are majorly used, dot-product compatibility function~\citep{LuongPM15}~\citep{vaswani2017attention} and multi-layer perceptron compatibility function~\citep{bah2015}. 

