With more and more literature produced in the biomedical area, building an efficient toolkit to automatically extract knowledge from documents becomes increasingly necessary. To handle this requirement, BioNER plays an important role that aims to recognize biomedical entities, such as genes, chemicals, and diseases without too many human efforts. Besides, it is also beneficial for many downstream applications, like relation extraction, event detection and document summarization. 

% Current work on BioNER, handcraft and nn based for feature encoding
Generally, BioNER is regarded as a sequence labeling task, which means we should assign the best label sequence to the input word sequence. Traditional methods often require handcrafted features to locate specific entity types. For instance, we can frequently see the suffix '-ase' more in proteins than diseases~\citep{habibi2017deep}. However, generating such amount of domain-specific features for BioNER system requires domain knowledge and cannot be directly adapted to recognize new entity types. Rather than manually designing entity-specific features, recent works mainly focus on developing an automatic learning algorithm to extract latent features of sequential words to feed a conditional random field (CRF)~\citep{lafferty2001conditional} layer to predict the output label sequence. \citet{habibi2017deep} followed the suggestion from \citet{lample2016neural} and proposed BiLSTM-CRF model to predict the label sequence with completely agnostic knowledge to every type of the entity. This neural network model only requires golden dataset with entity labels and pre-trained word embedding from large, domain-specific corpus (e.g. all PubMed abstracts). \citet{wang2018cross} improves the ability of BiLSTM-CRF network with multi-task learning by sharing BiLSTM layers .

% Development on feature encoding
Currently, with the fast development of attention mechanism~(\citet{vaswani2017attention},~\citet{bah2015},~\citet{LuongPM15}), researchers work on building new model architecture for feature encoding that eschews recurrence but instead relies on only the attention mechanism to encode the sequential information about input corpus. \citet{vaswani2017attention} proposed one novel sequence transduction model, called Transformer, based solely on attention mechanism. \citet{lee2019biobert} applied a pre-trained biomedical language representation model based on BERT architecture~\citep{devlin2018bert} which performs well in three representative biomedical text mining tasks, including BioNER. 

In this paper, we combine the idea of BiLSTM-CRF~\citep{habibi2017deep} model and self-attention mechanism~\citep{vaswani2017attention}, by replacing feature encoding layer of BiLSTM with a self attention layer~\citep{vaswani2017attention} to represent character level information and word level information. Outputs of those layers are then still fed to a CRF layer to do the final label sequence predictions. We call it Attention-CRF model. This method completely eschews the sequential computation, and only needs to parameterize the attention assigned on each unit (word or character) to encode the unit information. We evaluate and compare the performance of Attention-CRF with other 4 neural network architecture in 15 benchmark biomedical dataset used by \citep{crichton2017neural}.